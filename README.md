# 物性研スパコンハンズオン

## 概要

東京大学物性研究所(以下、物性研)に限らず、スパコンを保有する組織は、目的に合わせて複数のスパコンを持っているのが普通である。物性研は、2021年現在「システムB」と「システムC」と呼ばれる。

## スパコンの基礎

事前に[スパコンの使い方と諸注意](https://www.youtube.com/watch?v=YcsKEyK9G00)を見ておくこと。

## ログイン

アカウントは「kXXXXYY」という形になっている。XXXXがプロジェクト番号、YYがプロジェクト中のメンバー番号で、プロジェクトリーダーが「00」、以下登録順に「01」「02」と続く。渡辺研のプロジェクト番号はk0117であるため、渡辺のアカウント名は「k011700」である。

物性研スパコンのサーバ名は、システムBが「ohtaka」、システムCが「enaga」である。ドメインは「issp.u-tokyo.ac.jp」となる。例えば渡辺がシステムBにログインしたければ、

```sh
ssh k011700@ohtaka.issp.u-tokyo.ac.jp -AY
```

とすれば良い。ここで`-A`オプションはエージェント転送、`-Y`オプションは信頼されたX11の転送である。

## マニュアルの閲覧

マニュアル類は以下のところにある。

[物性研スパコン各種マニュアル](https://www.issp.u-tokyo.ac.jp/supercom/for-users/documents)

まず読むべき基本となるマニュアルは「利用の手引き」である。今回のハンズオンではシステムBを用いるので「利用の手引き(システムB編: 日本語)」を読む。パスワード認証がかかっているが、以下のIDとパスワードでアクセスできる。

* ユーザID：物性研スーパーコンピュータシステムのユーザ名
* パスワード：登録メールアドレスのユーザ名 (@より前の部分)

他にも利用者講習会(要パスワード)やスパコンの使い方と諸注意などの資料があるので、必要に応じて読むこと。

詳しい言語仕様やライブラリのマニュアルなどは「システム B マニュアル」などにある。こちらはかなり詳細な情報を含むので、最初は読まなくて良い。必要に応じて参照すること。

## コンパイルとジョブの投入

### コンパイル

MPIプログラムのコンパイルは`mpicxx`を用いる。

```sh
cd mpitest
mpicxx test.cpp
```

実行可能ファイル`a.out`が作成されるはずである。これを実行するためには、ジョブスケジューラに計算資源を割り当ててもらう必要がある。ジョブスケジューラには様々な種類があるが、物性研スパコンシステムBではSlurm、システムCではPBSProが使われており、それぞれコマンドが異なる。

### インタラクティブジョブ

ジョブの実行方法には、インタラクティブジョブとバッチジョブの二通りがある。まずはインタラクティブジョブによりジョブを実行しよう。インタラクティブジョブとは、専用に用意された計算資源をジョブスケジューラに割り当ててもらい、対話的にプログラムを実行するジョブである。物性研システムBには`i8cpu`というインタラクティブキューが用意されている。このキューを利用するには、以下のコマンドを実行する。

```sh
salloc -N 1 -n 128 -p i8cpu
```

`salloc`が計算資源要求をするためのコマンド、`-N 1`は、要求ノード数(ここでは1ノード)、`-n 128`は利用プロセス数(ここでは128プロセス)、`-p i8cpu`は計算資源(パーティション)の指定で、ここでは`i8cpu`を指定している。

実行すると以下のような表示がされる。

```txt
salloc: Pending job allocation 272941
salloc: job 272941 queued and waiting for resources
salloc: job 272941 has been allocated resources
salloc: Granted job allocation 272941
```

* これはジョブIDとして272941が割り振られ、
* リソースの割当を待っており(queued and waiting for resources)
* 計算資源が割り当てられ(has been allocated resources)
* ジョブの実行が開始された(Granted job allocation)

という意味だ。この状態でジョブを実行するには`srun`を用いる。

```sh
srun ./a.out
```

`srun`は、Slurmで管理されたシステムにおいて並列ジョブを実行するためのコマンドである。このジョブに割り当てられた計算資源を確認し、どのプロセスをどの計算資源に割り振るかを自動的に決定する。

実行終了したら`exit`によりインタラクティブジョブを抜けること。

### バッチジョブ

インタラクティブジョブは対話的に計算を行うが、これはデバッグや動作確認に用いるためのキューであり、一般の大きな計算はキューに積んで順番待ちをする。この順番待ちの列のことをキュー(Queue)と呼ぶ。プログラムをバッチジョブとして実行するためには、ジョブスケジューラにどんな計算資源を要求するか、どのようにプログラムを実行するのかを教えてやる必要がある。それを記述するのがジョブスクリプトである。ジョブスクリプトは、特殊なコメント欄を持つシェルスクリプトである。例えば、このリポジトリにはこんなジョブスクリプトが用意されている。

```sh
#!/bin/sh

#SBATCH -p i8cpu
#SBATCH -N 1
#SBATCH -n 128

srun ./a.out
```

シェルスクリプトは`#`以後はコメントとして扱われるが、このスクリプトでは全て意味を持っている。

最初の`#!`で始まる行はシバン(shebang)と呼ばれ、このスクリプトを処理するシェルを指定する。

次行からの`#SBATCH`で始まる行が、ジョブスケジューラ(Slurm)への指示である。それぞれの意味は以下の通り。

* `-p i8cpu` 計算資源(パーティション)の指定で、ここでは`i8cpu`を指定している。
* `-N 1` 要求ノード数(ここでは1ノード)
* `-n 128` 利用プロセス数(ここでは128プロセス)

これらは先程`salloc`にコマンド引数として渡したものと同じものである。コマンド引数として直接情報を渡すこともできるが、後でどんな計算資源を要求したかの情報がファイルに残るようにシェルスクリプトに記述しておいた方がよい。

また、PBSの場合はジョブ投入時のディレクトリが環境変数`PBS_O_WORKDIR`に入っており、コードの実行前に

```sh
cd $PBS_O_WORKDIR
```

を実行するのが一般的であるが、Slurmでは自動的に実行時のディレクトリをカレントディレクトリとするためこのコマンドは不要である。

このシェルスクリプトをジョブとして投入するには`sbatch`コマンドを使う。

```sh
sbatch test.sh
```

ジョブを投入した結果、ジョブIDが割り振られる。ジョブの状態は`squeue`コマンドで知ることができる。

```sh
squeue
```

例えば以下のような表示がでる。

```txt
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            272905     i8cpu  test.sh  k011700  R       0:03      1 c15u01n1 
```

それぞれの情報の意味は以下の通り。

* ジョブID(JOBID)として272905が割り振られた
* 計算リソース(PARTITION)は`i8cpu`を要求している
* シェルスクリプトのファイル名(NAME)は`test.sh`である
* 状態(status, ST)は実行中(R)である
* 実行開始からの時間(TIME)は3秒(0:03)
* 要求ノード数(NODES)は1である
* 実行ノードリスト(NODELIST)、もしくは待ち状態ならその理由(REASON)

実行が終わると、`slurm-ジョブID.out`というファイルが作成される。ここには標準出力が保存されている。今回はジョブIDが272905であるため、`slurm-272905.ou`というファイルができている。lessやcat等で中身を確認せよ。

### メール通知

スパコンが混んでいる場合、ジョブの実行には数日以上待たされる。そこで、ジョブが実行開始したり、実行を終了したらメールで通知する機能がある。Slurmではメール通知は`--mail-type`や`--mail-user`で指定する。

```sh
#!/bin/sh

#SBATCH -p i8cpu
#SBATCH -N 1
#SBATCH -n 128
#SBATCH --mail-type=BEGIN
#SBATCH --mail-user=your@mail.address

srun ./a.out
```

`test_mail.sh`をvimで開き、上記の`your@mail.address`を自身のアドレスに変えてから

```sh
sbatch test_mail.sh
```

を実行せよ。ジョブの開始時に`root@sched1.ohtaka.issp.u-tokyo.ac.jp`というアドレスからメール通知が飛んでくるはずである。

## スレッド並列やハイブリッド並列

### スレッド並列

並列化には大きく分けてプロセス並列、スレッド並列、データ並列がある。プロセス並列はMPIというライブラリで、スレッド並列はOpenMPというディレクティブで、データ並列は組み込み関数で実装するのが一般的だ。先程はMPIでプロセス並列を試したので、次はスレッド並列を試してみよう。OpenMPを使うには、コンパイラにそれを教えてやる必要がある。`g++`なら`-fopenmp`、インテルコンパイラ`icpc`なら`-qopenmp`である。ここではインテルコンパイラを使おう。

```sh
cd threadtest
icpc -qopenmp test.cpp
```

そのまま実行してみよう。

```sh
$ ./a.out 
000/001
```

スレッドが1つ作成され、スレッドID0番からの出力がなされた。スレッド数を変更するには環境変数`OMP_NUM_THREADS`を指定する。

```sh
$ OMP_NUM_THREADS=4 ./a.out
003/004
000/004
001/004
002/004
```

4スレッド起動したことがわかる。ログインノードであまりスレッドを立ち上げると迷惑になるのでこれくらいにしておこう。

これをジョブスケジューラにバッチジョブとして投げるには、「プロセス数」に加えて「プロセスあたりのスレッド数」も指定してやる必要がある。ジョブスクリプトはこんな感じになる。

```sh
#!/bin/sh

#SBATCH -p i8cpu
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 128

srun ./a.out
```

ジョブの投入は先程と同様である。

```sh
sbatch test.sh
```

### ハイブリッド並列

SIMDはさておくと、計算科学における並列計算とは複数のCPUコアを同時につかう計算のことである。この時、使うCPUの数と同じ数のプロセスを立ち上げるのをflat-MPIと呼び、CPUの数より少ないプロセスを立ち上げ、それぞれのプロセスに複数のスレッドを起動する並列をハイブリッド並列と呼ぶ。

物性研システムBは1ノードに64コアのCPUが2ソケットあるため、全体で128並列まで実行できる。これを8プロセス*16スレッドで実行してみよう。

```sh
cd hybrid
mpicxx -fopenmp test.cpp
sbatch test.sh
```

できたファイル`slurm-XXXXX.out`をsortコマンドで表示してみよう。

```sh
$ sort slurm-272988.out
Thread:000/016 Process:000/008
Thread:000/016 Process:001/008
Thread:000/016 Process:002/008
Thread:000/016 Process:003/008
(snip)
Thread:015/016 Process:004/008
Thread:015/016 Process:005/008
Thread:015/016 Process:006/008
Thread:015/016 Process:007/008
```

プロセスが8個、それぞれのプロセスにおいてスレッドが16個起動していることがわかるであろう。

## LAMMPSの利用

システムBではビルド済みのアプリケーションが多数インストールされている。インストール済みアプリケーションの一覧は「[インストール済みアプリケーション](https://www.issp.u-tokyo.ac.jp/supercom/visitor/applications)」を参照のこと。

アプリケーションを利用するには、それぞれのディレクトリにある アプリケーション名vars.shを実行する。LAMMPSであれば、

```sh
source /home/issp/materiapps/intel/lammps/lammpsvars.sh
```

を実行することで`lammps`にパスが通る。

これを物性研で実行してみよう。

```sh
cd lammps
python3 generate_config.py
```

これで`collision.atoms`が作成されるはずである。この状態でインタラクティブキューを起動しよう。

```sh
salloc -N 1 -n 128 -p i8cpu
```

ジョブが開始したら、パスを通してから実行する。

```sh
source /home/issp/materiapps/intel/lammps/lammpsvars.sh
srun lammps < collision.input 
```

最初にこんな表示がされるはずだ。

```sh
LAMMPS (29 Oct 2020)
  using 1 OpenMP thread(s) per MPI task
Reading data file ...
  orthogonal box = (-40.000000 -20.000000 -20.000000) to (40.000000 20.000000 20.000000)
  8 by 4 by 4 MPI processor grid
```

これは、1プロセスに対して1スレッドのflat-MPIであり、空間を8x4x4に区切って、それぞれをプロセスに任せることで128プロセスの計算をするよ、という意味だ。

このジョブでは原子数が少ないこともあって並列化による性能向上はあまり見込めないが、大きな系を計算する際には並列化は非常に有効である。

せっかくなのでハイブリッド実行もためそう。こんなジョブスクリプトを用意してある。

```sh
#!/bin/sh

#SBATCH -p i8cpu
#SBATCH -N 1
#SBATCH -n 8
#SBATCH -c 16

source /home/issp/materiapps/intel/lammps/lammpsvars.sh
srun lammps < collision.input
```

これは、8プロセス*16スレッドの並列実行ジョブだ。パスはジョブスクリプトの中で通す必要があることに注意。

実行するとこんな結果が得られるはず。

```txt
LAMMPS (29 Oct 2020)
  using 16 OpenMP thread(s) per MPI task
Reading data file ...
  orthogonal box = (-40.000000 -20.000000 -20.000000) to (40.000000 20.000000 20.000000)
  2 by 2 by 2 MPI processor grid
```

1プロセスあたり16スレッド、空間を2x2x2に分割したことを示している。
